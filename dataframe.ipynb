{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Initial Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iter = 3000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>metadata</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_57169</td>\n",
       "      <td>[-0.03923165, -0.0016832227, 0.014283108, -0.0...</td>\n",
       "      <td>{'chunkTitle': 'Muscle Hypertrophy', 'episodeT...</td>\n",
       "      <td>Andrew Huberman: And that leads me to a questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_57178</td>\n",
       "      <td>[-0.02918509, -0.018408643, 0.017951787, -0.03...</td>\n",
       "      <td>{'chunkTitle': 'Hypertrophy Training Overview'...</td>\n",
       "      <td>Andrew Huberman: So as you point out before an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_57177</td>\n",
       "      <td>[-0.030115174, -0.01321885, 0.02527428, -0.005...</td>\n",
       "      <td>{'chunkTitle': 'Fitness and Hypertrophy', 'epi...</td>\n",
       "      <td>Andy Galpin: Sure. So we have a lot less infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_57176</td>\n",
       "      <td>[-0.028269125, 0.006626837, 0.022055835, -0.00...</td>\n",
       "      <td>{'chunkTitle': 'Cardio and Hypertrophy', 'epis...</td>\n",
       "      <td>Andrew Huberman: How do other forms of exercis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_57175</td>\n",
       "      <td>[-0.025013695, 0.00445241, 0.029148076, -0.009...</td>\n",
       "      <td>{'chunkTitle': 'Indicators and Adjustments', '...</td>\n",
       "      <td>Andy Galpin: You could also look at things lik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                          embedding  \\\n",
       "0  chunk_57169  [-0.03923165, -0.0016832227, 0.014283108, -0.0...   \n",
       "1  chunk_57178  [-0.02918509, -0.018408643, 0.017951787, -0.03...   \n",
       "2  chunk_57177  [-0.030115174, -0.01321885, 0.02527428, -0.005...   \n",
       "3  chunk_57176  [-0.028269125, 0.006626837, 0.022055835, -0.00...   \n",
       "4  chunk_57175  [-0.025013695, 0.00445241, 0.029148076, -0.009...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'chunkTitle': 'Muscle Hypertrophy', 'episodeT...   \n",
       "1  {'chunkTitle': 'Hypertrophy Training Overview'...   \n",
       "2  {'chunkTitle': 'Fitness and Hypertrophy', 'epi...   \n",
       "3  {'chunkTitle': 'Cardio and Hypertrophy', 'epis...   \n",
       "4  {'chunkTitle': 'Indicators and Adjustments', '...   \n",
       "\n",
       "                                            document  \n",
       "0  Andrew Huberman: And that leads me to a questi...  \n",
       "1  Andrew Huberman: So as you point out before an...  \n",
       "2  Andy Galpin: Sure. So we have a lot less infor...  \n",
       "3  Andrew Huberman: How do other forms of exercis...  \n",
       "4  Andy Galpin: You could also look at things lik...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/dexaai/huberman_on_exercise/data/data-00000-of-00001-8e5e40fbf9236004.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Embedding and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_57169</td>\n",
       "      <td>{'chunkTitle': 'Muscle Hypertrophy', 'episodeT...</td>\n",
       "      <td>Andrew Huberman: And that leads me to a questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_57178</td>\n",
       "      <td>{'chunkTitle': 'Hypertrophy Training Overview'...</td>\n",
       "      <td>Andrew Huberman: So as you point out before an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_57177</td>\n",
       "      <td>{'chunkTitle': 'Fitness and Hypertrophy', 'epi...</td>\n",
       "      <td>Andy Galpin: Sure. So we have a lot less infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_57176</td>\n",
       "      <td>{'chunkTitle': 'Cardio and Hypertrophy', 'epis...</td>\n",
       "      <td>Andrew Huberman: How do other forms of exercis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_57175</td>\n",
       "      <td>{'chunkTitle': 'Indicators and Adjustments', '...</td>\n",
       "      <td>Andy Galpin: You could also look at things lik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           metadata  \\\n",
       "0  chunk_57169  {'chunkTitle': 'Muscle Hypertrophy', 'episodeT...   \n",
       "1  chunk_57178  {'chunkTitle': 'Hypertrophy Training Overview'...   \n",
       "2  chunk_57177  {'chunkTitle': 'Fitness and Hypertrophy', 'epi...   \n",
       "3  chunk_57176  {'chunkTitle': 'Cardio and Hypertrophy', 'epis...   \n",
       "4  chunk_57175  {'chunkTitle': 'Indicators and Adjustments', '...   \n",
       "\n",
       "                                            document  \n",
       "0  Andrew Huberman: And that leads me to a questi...  \n",
       "1  Andrew Huberman: So as you point out before an...  \n",
       "2  Andy Galpin: Sure. So we have a lot less infor...  \n",
       "3  Andrew Huberman: How do other forms of exercis...  \n",
       "4  Andy Galpin: You could also look at things lik...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Second Column (Embedding)\n",
    "df = df.drop(columns=['embedding'])\n",
    "\n",
    "import re\n",
    "# Remove punctuation (keeps numbers and capitalization)\n",
    "# df['document'] = df['document'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Andrew Huberman: And that leads me to a question that is based on findings that I've heard discussed on social media, which means very little, if anything, unless it's in the context of people who really know exercise science. And you're one such person. And that's this idea that because resistance training can evoke a protein synthesis adaptation response, but that adaptation response lasts about 48 hours before it starts to taper off. That the ideal. In quotes, frequency for training a given muscle group for hypertrophy is about every 48 hours. Is that true?\\n\\nAndy Galpin: Yes and no. So a couple of things there remember, in order to grow a muscle, there's multiple steps here. So you have the signaling response, which actually happens within seconds of exercise and can last, depending on the marker, up to an hour or 2 hours. Step number two then is gene expression, and we see that that's typically peaked around two to 6 hours post exercise. And then you have following that protein synthesis and that's that longer time frame somewhere between 12 hours there. It's certainly not peaked for 48 hours. It may be still there 48 hours from now, but it is absolutely coming down at that point depending on sort of a number of factors. So that part of it is sort of true. So this is a combination of some half truths and some maybe just pedantic things that aren't really that important to differentiate. The real question, I think, is, like, okay, is it okay to train sooner? Is it better to train sooner? Or actually, is it better to wait longer?\\n\\nAndy Galpin: There's no real reason to think that you need to train if the goal is hypertrophy any sooner than 48 hours afterwards. I can't think of an advantage that that would confer. I also can't think of any practical applications, athletes, physique, bodybuilders coaches that ever found tremendous success doing that. So I would be very skeptical that that is in any way better. Now, could you do it in some instances of, say, you've got travel coming up like that so that you just yeah.\\n\\nAndrew Huberman: You want to preload the system by.\\n\\nAndy Galpin: Destroying the total no problem.\\n\\nAndrew Huberman: And then waiting seven days or 14 days. I've known people have done that before.\\n\\nAndy Galpin: I do vacations or layoffs every time.\\n\\nAndrew Huberman: Like every single annihilate themselves and then take a week layoff.\\n\\nAndy Galpin: Yeah. Andy it's like there's no benefit there other than psychological. Like I just love it. It feels great to be super sore. I feel less crappy not training for those couple of days because I'm like, I'm super sore anyway.\\n\\nAndrew Huberman: You need the extended rest.\\n\\nAndy Galpin: Yeah, of course. And it's just a crappy justification in my brain, that excuse to do something really wild and that I totally don't need. Andy get way sore than I should.\\n\\nAndrew Huberman: Get Dr. Andy Galvin's suggestions of what not to do, but that he does.\\n\\nAndy Galpin: Yeah, 100%.\",\n",
       " \"Andrew Huberman: So as you point out before and I can only assume you're referring to me, hypertrophy training is idiot proof, meaning there's a lot of leeway in the variables but not so much leeway that people can do anything. It's bounded by these general principles. So, with your permission, I'm going to do a brief overview of my notes based on your description of the modifiable variables that will direct somebody towards hypertrophy. Keeping in mind this backdrop of exercise choice exercise order selecting appropriate volume that sets and reps training frequency and needing some metric or way to have progression either by adding more weights or by more tension or more metabolic stress and so on. In terms of exercise choice. It sounds like the choice of exercise science not super critical in terms of specificity, but that the ideal circumstances. That people are targeting all the major and, frankly, secondary and minor muscle groups, if you can even call them.\\n\\nAndrew Huberman: That across their exercise choices, that they're picking exercises that they can perform safely and that they can generate enough intensity so that they're getting close to failure without placing themselves into danger.\\n\\nAndy Galpin: Right?\\n\\nAndrew Huberman: So for some people that might mean including large compound free weight exercises like squats and deadlifts andy bent over barbell rows as well as isolation exercises. And for some people there might be a bias toward more isolation exercise science machines. But of course machines don't necessarily mean that you can't use heavy loads. In fact, plateleted machines like Hammer strength machines will allow for quite substantial loads. So picking two or three or more movements per muscle group can be valuable. But that overall consistency is going to outshine variation in the sense that you don't need to hit muscles with a different exercise every workout coming back to the same things has a benefit. And we heard about this in our discussion around strength and power as well. Okay. In terms of order of exercise science too, it sounds like there's a lot of flexibility.\\n\\nAndrew Huberman: One could do the large compound exercise for, let's say, quadriceps and hamstrings and glutes first, like a squat or a front squat or could deadlift for that matter. But then if one deadlifted andy primarily hit the glutes and hamstrings, then you might want to target the quadriceps more directly with leg extensions. Or if one squatted and was loading the squat bar, carrying the squat bar in a way that was predominantly quadricep and less so glute and hamstring, then leg curls would be a good choice, et cetera. Okay, Andy, train your calves, folks. Very important. Unless you're a genetic freak, of course. It's actually a good opportunity to say unless you're a genetic freak or you just have a genetic predisposition or you've done sports, Andy, you have a genetic predisposition that gives you very large calves that don't require any training at all. I know people like this, they're somewhat rare, but they're out there. And those folks sometimes want to stay away from or minimize their training.\",\n",
       " \"Andy Galpin: Sure. So we have a lot less information on the potential interference or not of high intensity stuff. The stuff we do have suggested it may actually aid in hypertrophy. And that's because if you think about it, one of the potential paths to activation Andy muscle growth is this metabolic disturbance. You're going to get that a lot with the high intensity interval thing. So it's not a terrible thing to do. I wouldn't do it to the level that it compromises your ability to come back. Andy, do your primary training. So if you're so fatigued, your legs are super heavy, they're depleted. You now have to ingest extra carbohydrates to replenish muscle glycogen to be able to handle both recovery and continued training, et cetera. That could then lead to a problem. But in general, we really don't see any reason why that is going to completely block or make it such that your training was quote unquote, wasted or it didn't work.\\n\\nAndy Galpin: In fact, actually a very recent study came out where they had individuals perform six weeks of purely aerobic endurance, steady state, long duration endurance for six weeks. I think prior to starting a hypertrophy phase compared that to individuals who did not do that. And those folks that did these six weeks of just, I think it was cycling, actually just endurance work, had more muscle growth at the end of their hypertrophy training than those folks that did not. So this shows you very clearly there are a lot of advantages that come with being physically fit to growing muscle. So folks that also have actually hit plateaus a lot, one of the things you may actually see some benefit from is actually doing a little bit more endurance work. Whether it's a steady state stuff, maybe it's the higher intensity stuff. Certainly if you're starting a training phase, it's a pretty good idea to do that. And there's a number of physiological reasons of why that's potentially occurring. But the lowest hanging fruit here is we sort of joke if you're so unfit that you're tying your shoes in your warm up and you're already breaking a sweat, you probably don't have enough fitness to do enough training to get enough hypertrophy. So that is in fact your limiting factor.\\n\\nAndy Galpin: You're not recovering. You're super fatigued andy damaged and sore because you're so unfit. So get fit first and then you can actually get more gains a week later. So you have to kind of kick the can down the road for a few weeks, but ten weeks later you'll be in a better spot than you were by investing a little bit in your conditioning.\",\n",
       " \"Andrew Huberman: How do other forms of exercise combine with hypertrophy training? For instance, can I do cardiovascular training for two or three days per week, provided that cardiovascular training is of low enough intensity and not disrupt hypertrophy progression. And can I do that cardiovascular exercise before or after the hypertrophy training, or does it need to be separated out?\\n\\nAndy Galpin: The answer to this is really what we call the crossover interference effect. It's really an energy management issue. So the only time endurance exercise starts to interfere or block or hinder attenuate hypertrophy is in one of two broad categories. Number one, total energy intake. Or your balance is off. So you can ameliorate this by just eating more. If you do that, then the interference effect generally goes away. The second one is you want to make sure you avoid exercise forms for your endurance training that are the same working group and specifically the eccentric portion. So for example, we see much more interference with running on leg hypertrophy than we do with cycling, right? Less eccentric pounding and loading, less damage, less things to recover from. The tissue seems to be totally fine. The only other thing you need to worry about here is total volume of your endurance work. So if you're doing a moderate intensity for a moderate duration, say 70% of your maximum heart rate for 25 minutes, it's unlikely to do much damage.\\n\\nAndy Galpin: In terms of blocking hypertrophy, you're totally fine. Can you do it before or after your workout? It's probably not going to matter that much. So pre fatigue is okay for hypertrophy. So if your pre fatigue is coming from endurance, then you're totally fine. Not a big deal afterwards. Cool. You want to break it up into multiple sessions, that's probably better, right? So if you do your endurance work on a separate day, that's probably best case scenario. If you can't do that but you can break it up into two workouts. Say you lift in the morning and then you do your quote unquote cardio at night. Maybe that's second best. Third best is doing it at the end of your lift. Andy finishing it, that's fine. Just make sure that you're maximizing your recovery on all the other tricks we'll talk about later. Make sure the calories are there, make sure you're not doing a lot of eccentric landing in that endurance stuff and you'll be just fine.\\n\\nAndrew Huberman: And where does higher intensity cardio fit into a hypertrophy program? So higher intensity cardio, for instance, in my mind, is getting on the assault bike and doing eight intervals of 22nd sprints and ten second rest in between, or perhaps going to a field and doing some bounds and sprints and things of that sort. Not going all out, not running for one's life, but getting up to about 85, 90% of running for one's life.\",\n",
       " \"Andy Galpin: You could also look at things like Hrv heart rate variability, which is a very classic marker and much more sensitive to changes with training than something like a resting heart rate, which is one thing you can actually do that's totally cost free. Just look at your changes. And any elevation resting heart rate over time, especially more than three to five consecutive days, is an indicator. But Hrv is much more sensitive to things like training induced overload. So that's a quick version of stuff that we're going to pay attention to. The last one, I would add there is simply motivation. So if you're really training hard and you like training hard and you just cannot force yourself to go anymore, that in Andy of itself can be a good indication of it's. Maybe not the day, maybe not the week. With all of these things you want to be careful about overreacting to a single day measure. Again, we need to look at at least a trend of more than three days. Honestly, I'm looking at more than five days.\\n\\nAndy Galpin: I'm going to pull back from that and think about what phase of training we're in, what part of the year we're in, typically with our athletes in season, preseason, postseason offseason, et cetera, to make our decisions about what we're going to do about it? Are we canning the entire workout? Are we doing a modified lower version, lower intensity my default. Generally if hypertrophy is the goal, remember, volume is the driver there. So if I can, can we get in? Can we go real light? Let's go to six out of ten Rpe. So relative perceived exertion, maybe we'll reduce the range of motion, maybe we'll make it a little bit easier, maybe go to machines or instead of going a squat, we'll just do leg extension, something like that. But I want to still get enough volume in there that will keep you on target again, even going at 50%, not to high repetition, 50% for a set of ten, three sets.\\n\\nAndy Galpin: Just get a nice blood flow in there, get it in, get it out, aid in recovery and then move on and come back the next day. That's probably what I would do. Rather than canning the entire session.\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to list in order to combine all documents into one list\n",
    "corpus = df['document'].tolist()\n",
    "# print(type(corpus))\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812479\n"
     ]
    }
   ],
   "source": [
    "# Convert list to string by joining all documents\n",
    "text = \" \".join(corpus)\n",
    "# print(text[:1000])\n",
    "print(len(text)) # The number of chars in the corpus\n",
    "# print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 76\n",
      "Vocabulary in text:  \n",
      " \"$%',-./0123456789:?@ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz£\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all unique characters in the text\n",
    "vocabulary = sorted(list(set(text)))\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "print('Vocabulary Size: ', vocabulary_size) # The number of unique characters in the text\n",
    "print('Vocabulary in text: ', ''.join(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization\n",
    "\n",
    "#### Convert tokens into numerical representations. Starting with:\n",
    "#### Bag-of-Words (BoW)\n",
    "#### TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# print(tiktoken.list_encoding_names())\n",
    "encoder = tiktoken.get_encoding('o200k_base')\n",
    "# assert encoder.decode(encoder.encode(\"hello world\")) == \"hello world\"\n",
    "print(encoder.decode(encoder.encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [49, 62, 52, 66, 53, 71, 1, 56, 69, 50, 53, 66, 61, 49, 62]\n",
      "Decoded: andrew huberman\n"
     ]
    }
   ],
   "source": [
    "# Map each character to an index\n",
    "stoi = {c: i for i, c in enumerate(vocabulary)}\n",
    "itos = {i: c for i, c in enumerate(vocabulary)}\n",
    "encode = lambda x: [stoi[c] for c in x] # encode('a') -> 0, takes a character and returns its index in the vocabulary\n",
    "decode = lambda s: ''.join(itos[i] for i in s) # decode([0, 1, 2]) -> 'abc', takes a list of indices and returns the corresponding string\n",
    "print('Encoded:', encode(\"andrew huberman\"))\n",
    "print('Decoded:', decode(encode(\"andrew huberman\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([812479]) torch.LongTensor\n",
      "tensor([23, 62, 52, 66, 53, 71,  1, 30, 69, 50, 53, 66, 61, 49, 62, 20,  1, 23,\n",
      "        62, 52,  1, 68, 56, 49, 68,  1, 60, 53, 49, 52, 67,  1, 61, 53,  1, 68,\n",
      "        63,  1, 49,  1, 65, 69, 53, 67, 68, 57, 63, 62,  1, 68, 56, 49, 68,  1,\n",
      "        57, 67,  1, 50, 49, 67, 53, 52,  1, 63, 62,  1, 54, 57, 62, 52, 57, 62,\n",
      "        55, 67,  1, 68, 56, 49, 68,  1, 31,  5, 70, 53,  1, 56, 53, 49, 66, 52,\n",
      "         1, 52, 57, 67, 51, 69, 67, 67, 53, 52,  1, 63, 62,  1, 67, 63, 51, 57,\n",
      "        49, 60,  1, 61, 53, 52, 57, 49,  6,  1, 71, 56, 57, 51, 56,  1, 61, 53,\n",
      "        49, 62, 67,  1, 70, 53, 66, 73,  1, 60, 57, 68, 68, 60, 53,  6,  1, 57,\n",
      "        54,  1, 49, 62, 73, 68, 56, 57, 62, 55,  6,  1, 69, 62, 60, 53, 67, 67,\n",
      "         1, 57, 68,  5, 67,  1, 57, 62,  1, 68, 56, 53,  1, 51, 63, 62, 68, 53,\n",
      "        72, 68,  1, 63, 54,  1, 64, 53, 63, 64, 60, 53,  1, 71, 56, 63,  1, 66,\n",
      "        53, 49, 60, 60, 73,  1, 59, 62, 63, 71,  1, 53, 72, 53, 66, 51, 57, 67,\n",
      "        53,  1, 67, 51, 57, 53, 62, 51, 53,  8,  1, 23, 62, 52,  1, 73, 63, 69,\n",
      "         5, 66, 53,  1, 63, 62, 53,  1, 67, 69, 51, 56,  1, 64, 53, 66, 67, 63,\n",
      "        62,  8,  1, 23, 62, 52,  1, 68, 56, 49, 68,  5, 67,  1, 68, 56, 57, 67,\n",
      "         1, 57, 52, 53, 49,  1, 68, 56, 49, 68,  1, 50, 53, 51, 49, 69, 67, 53,\n",
      "         1, 66, 53, 67, 57, 67, 68, 49, 62, 51, 53,  1, 68, 66, 49, 57, 62, 57,\n",
      "        62, 55,  1, 51, 49, 62,  1, 53, 70, 63, 59, 53,  1, 49,  1, 64, 66, 63,\n",
      "        68, 53, 57, 62,  1, 67, 73, 62, 68, 56, 53, 67, 57, 67,  1, 49, 52, 49,\n",
      "        64, 68, 49, 68, 57, 63, 62,  1, 66, 53, 67, 64, 63, 62, 67, 53,  6,  1,\n",
      "        50, 69, 68,  1, 68, 56, 49, 68,  1, 49, 52, 49, 64, 68, 49, 68, 57, 63,\n",
      "        62,  1, 66, 53, 67, 64, 63, 62, 67, 53,  1, 60, 49, 67, 68, 67,  1, 49,\n",
      "        50, 63, 69, 68,  1, 14, 18,  1, 56, 63, 69, 66, 67,  1, 50, 53, 54, 63,\n",
      "        66, 53,  1, 57, 68,  1, 67, 68, 49, 66, 68, 67,  1, 68, 63,  1, 68, 49,\n",
      "        64, 53, 66,  1, 63, 54, 54,  8,  1, 42, 56, 49, 68,  1, 68, 56, 53,  1,\n",
      "        57, 52, 53, 49, 60,  8,  1, 31, 62,  1, 65, 69, 63, 68, 53, 67,  6,  1,\n",
      "        54, 66, 53, 65, 69, 53, 62, 51, 73,  1, 54, 63, 66,  1, 68, 66, 49, 57,\n",
      "        62, 57, 62, 55,  1, 49,  1, 55, 57, 70, 53, 62,  1, 61, 69, 67, 51, 60,\n",
      "        53,  1, 55, 66, 63, 69, 64,  1, 54, 63, 66,  1, 56, 73, 64, 53, 66, 68,\n",
      "        66, 63, 64, 56, 73,  1, 57, 67,  1, 49, 50, 63, 69, 68,  1, 53, 70, 53,\n",
      "        66, 73,  1, 14, 18,  1, 56, 63, 69, 66, 67,  8,  1, 31, 67,  1, 68, 56,\n",
      "        49, 68,  1, 68, 66, 69, 53, 21,  0,  0, 23, 62, 52, 73,  1, 29, 49, 60,\n",
      "        64, 57, 62, 20,  1, 47, 53, 67,  1, 49, 62, 52,  1, 62, 63,  8,  1, 41,\n",
      "        63,  1, 49,  1, 51, 63, 69, 64, 60, 53,  1, 63, 54,  1, 68, 56, 57, 62,\n",
      "        55, 67,  1, 68, 56, 53, 66, 53,  1, 66, 53, 61, 53, 61, 50, 53, 66,  6,\n",
      "         1, 57, 62,  1, 63, 66, 52, 53, 66,  1, 68, 63,  1, 55, 66, 63, 71,  1,\n",
      "        49,  1, 61, 69, 67, 51, 60, 53,  6,  1, 68, 56, 53, 66, 53,  5, 67,  1,\n",
      "        61, 69, 60, 68, 57, 64, 60, 53,  1, 67, 68, 53, 64, 67,  1, 56, 53, 66,\n",
      "        53,  8,  1, 41, 63,  1, 73, 63, 69,  1, 56, 49, 70, 53,  1, 68, 56, 53,\n",
      "         1, 67, 57, 55, 62, 49, 60, 57, 62, 55,  1, 66, 53, 67, 64, 63, 62, 67,\n",
      "        53,  6,  1, 71, 56, 57, 51, 56,  1, 49, 51, 68, 69, 49, 60, 60, 73,  1,\n",
      "        56, 49, 64, 64, 53, 62, 67,  1, 71, 57, 68, 56, 57, 62,  1, 67, 53, 51,\n",
      "        63, 62, 52, 67,  1, 63, 54,  1, 53, 72, 53, 66, 51, 57, 67, 53,  1, 49,\n",
      "        62, 52,  1, 51, 49, 62,  1, 60, 49, 67, 68,  6,  1, 52, 53, 64, 53, 62,\n",
      "        52, 57, 62, 55,  1, 63, 62,  1, 68, 56, 53,  1, 61, 49, 66, 59, 53, 66,\n",
      "         6,  1, 69, 64,  1, 68, 63,  1, 49, 62,  1, 56, 63, 69, 66,  1, 63, 66,\n",
      "         1, 12,  1, 56, 63, 69, 66, 67,  8,  1, 41, 68, 53, 64,  1, 62, 69, 61,\n",
      "        50, 53, 66,  1, 68, 71, 63,  1, 68, 56, 53, 62,  1, 57, 67,  1, 55, 53,\n",
      "        62, 53,  1, 53, 72, 64, 66, 53, 67, 67, 57, 63, 62,  6,  1, 49, 62, 52,\n",
      "         1, 71, 53,  1, 67, 53, 53,  1, 68, 56, 49, 68,  1, 68, 56, 49, 68,  5,\n",
      "        67,  1, 68, 73, 64, 57, 51, 49, 60, 60, 73,  1, 64, 53, 49, 59, 53, 52,\n",
      "         1, 49, 66, 63, 69, 62, 52,  1, 68, 71, 63,  1, 68, 63,  1, 16,  1, 56,\n",
      "        63, 69, 66, 67,  1, 64, 63, 67, 68,  1, 53, 72, 53, 66, 51, 57, 67, 53,\n",
      "         8,  1, 23, 62, 52,  1, 68, 56, 53, 62,  1, 73, 63, 69,  1, 56, 49, 70,\n",
      "        53,  1, 54, 63, 60, 60, 63, 71, 57, 62, 55,  1, 68, 56, 49, 68,  1, 64,\n",
      "        66, 63, 68, 53, 57, 62,  1, 67, 73, 62])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text with torch\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.type())\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 731231\n",
      "Val size: 81248\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(data))\n",
    "val_data = data[train_size:]\n",
    "train_data = data[:train_size]\n",
    "print('Train size:', len(train_data))\n",
    "print('Val size:', len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23, 62, 52, 66, 53, 71,  1, 30])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([23]) the target is 62\n",
      "when input is tensor([23, 62]) the target is 52\n",
      "when input is tensor([23, 62, 52]) the target is 66\n",
      "when input is tensor([23, 62, 52, 66]) the target is 53\n",
      "when input is tensor([23, 62, 52, 66, 53]) the target is 71\n",
      "when input is tensor([23, 62, 52, 66, 53, 71]) the target is 1\n",
      "when input is tensor([23, 62, 52, 66, 53, 71,  1]) the target is 30\n",
      "when input is tensor([23, 62, 52, 66, 53, 71,  1, 30]) the target is 69\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  torch.Size([4, 8])\n",
      "tensor([[63, 66, 59,  1, 63, 66,  1, 67],\n",
      "        [68, 63,  1, 52, 63,  1, 68, 56],\n",
      "        [53,  1, 49,  1, 67, 53, 62, 67],\n",
      "        [68, 53, 57, 62,  1, 49, 62, 52]])\n",
      "Targets:  torch.Size([4, 8])\n",
      "tensor([[66, 59,  1, 63, 66,  1, 67, 57],\n",
      "        [63,  1, 52, 63,  1, 68, 56, 66],\n",
      "        [ 1, 49,  1, 67, 53, 62, 67, 53],\n",
      "        [53, 57, 62,  1, 49, 62, 52,  1]])\n",
      "------\n",
      "when input is [63] the target is 66\n",
      "when input is [63, 66] the target is 59\n",
      "when input is [63, 66, 59] the target is 1\n",
      "when input is [63, 66, 59, 1] the target is 63\n",
      "when input is [63, 66, 59, 1, 63] the target is 66\n",
      "when input is [63, 66, 59, 1, 63, 66] the target is 1\n",
      "when input is [63, 66, 59, 1, 63, 66, 1] the target is 67\n",
      "when input is [63, 66, 59, 1, 63, 66, 1, 67] the target is 57\n",
      "when input is [68] the target is 63\n",
      "when input is [68, 63] the target is 1\n",
      "when input is [68, 63, 1] the target is 52\n",
      "when input is [68, 63, 1, 52] the target is 63\n",
      "when input is [68, 63, 1, 52, 63] the target is 1\n",
      "when input is [68, 63, 1, 52, 63, 1] the target is 68\n",
      "when input is [68, 63, 1, 52, 63, 1, 68] the target is 56\n",
      "when input is [68, 63, 1, 52, 63, 1, 68, 56] the target is 66\n",
      "when input is [53] the target is 1\n",
      "when input is [53, 1] the target is 49\n",
      "when input is [53, 1, 49] the target is 1\n",
      "when input is [53, 1, 49, 1] the target is 67\n",
      "when input is [53, 1, 49, 1, 67] the target is 53\n",
      "when input is [53, 1, 49, 1, 67, 53] the target is 62\n",
      "when input is [53, 1, 49, 1, 67, 53, 62] the target is 67\n",
      "when input is [53, 1, 49, 1, 67, 53, 62, 67] the target is 53\n",
      "when input is [68] the target is 53\n",
      "when input is [68, 53] the target is 57\n",
      "when input is [68, 53, 57] the target is 62\n",
      "when input is [68, 53, 57, 62] the target is 1\n",
      "when input is [68, 53, 57, 62, 1] the target is 49\n",
      "when input is [68, 53, 57, 62, 1, 49] the target is 62\n",
      "when input is [68, 53, 57, 62, 1, 49, 62] the target is 52\n",
      "when input is [68, 53, 57, 62, 1, 49, 62, 52] the target is 1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(47)\n",
    "batch_size = 4 # number of sequences in a batch, processed in parallel\n",
    "block_size = 8 # length of a sequence, max context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train') \n",
    "print('Inputs: ', xb.shape)\n",
    "print(xb)\n",
    "print('Targets: ', yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('------')\n",
    "\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[63, 66, 59,  1, 63, 66,  1, 67],\n",
      "        [68, 63,  1, 52, 63,  1, 68, 56],\n",
      "        [53,  1, 49,  1, 67, 53, 62, 67],\n",
      "        [68, 53, 57, 62,  1, 49, 62, 52]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 76])\n",
      "tensor(4.9399, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "@IDh?MJoCJwVsrbMqbJq£gQMGUMFPAQVj?UpX£w0QpF.$xunu3BlM8qt8Vzy$9xJCPl2\"MI'sxXY%I:SHbGR%q@D3U'd,w?dYzww\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C) # batch, time, channels\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generation(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last token/time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocabulary_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # -ln(1/vocabulary_size) = -ln(1/76) = 4.3307\n",
    "\n",
    "print(decode(model.generation(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.434229612350464\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=None)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And st? Hus the vefe cey e oreagor y mik a hin f w ypind o w chet t izou mike, I 24 axhacoubest s bit owass nd ciatho te fat, ry, lins nteimar wevevaft'rik 7n or ove. rart e me the s qu byboves m rimasthaindim Sorand swhe thenlss, s st dicestup: s o trig I cesof toitoulinc t m, wiboing t. s, t, tong\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generation(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def generate_response(user_input):\n",
    "    # Placeholder function for generating a response\n",
    "    return \"Bot: \" + user_input[::-1] # We'll just reverse the input for now, and then put the variable name or method name to execute the response of the bot\n",
    "\n",
    "def respond():\n",
    "    user_input = entry.get()\n",
    "    if user_input.strip() == \"\" or user_input == placeholder_text:\n",
    "        return  # Ignore empty input or placeholder text\n",
    "    response = generate_response(user_input)\n",
    "    output.insert(tk.END, f\"You: {user_input}\\n{response}\\n\")\n",
    "    entry.delete(0, tk.END)  # Clear the entry box\n",
    "    window.focus()  # Shift focus to the main window to enable placeholder functionality\n",
    "    add_placeholder()  # Re-add placeholder immediately\n",
    "\n",
    "def add_placeholder():\n",
    "    \"\"\"Adds the placeholder text if the entry is empty.\"\"\"\n",
    "    if not entry.get():  # Check if the entry is empty\n",
    "        entry.insert(0, placeholder_text)\n",
    "        entry.config(fg=\"grey\")\n",
    "\n",
    "def remove_placeholder(event=None):\n",
    "    \"\"\"Removes the placeholder text when the user focuses on the entry.\"\"\"\n",
    "    if entry.get() == placeholder_text:\n",
    "        entry.delete(0, tk.END)\n",
    "        entry.config(fg=\"white\")  # Switch text color for actual input\n",
    "\n",
    "# Main window setup\n",
    "window = tk.Tk()\n",
    "window.title(\"Chatbot\")\n",
    "window.geometry(\"400x300\")\n",
    "\n",
    "placeholder_text = \"Ask a question\"\n",
    "\n",
    "# Output textbox (aligned at the top)\n",
    "output = tk.Text(window, wrap=\"word\", height=15)\n",
    "output.grid(row=0, column=0, columnspan=2, sticky=\"nsew\", padx=10, pady=(10, 0))\n",
    "\n",
    "# Entry box with placeholder\n",
    "entry = tk.Entry(window, fg=\"grey\")\n",
    "entry.grid(row=1, column=0, sticky=\"ew\", padx=(10, 5), pady=10)\n",
    "entry.insert(0, placeholder_text)  # Add placeholder initially\n",
    "entry.bind(\"<FocusIn>\", remove_placeholder)\n",
    "entry.bind(\"<FocusOut>\", lambda event: add_placeholder())\n",
    "\n",
    "# Submit button\n",
    "submit = tk.Button(window, text=\"Ask\", command=respond)\n",
    "submit.grid(row=1, column=1, sticky=\"ew\", padx=(5, 10), pady=10)\n",
    "\n",
    "# Configure grid layout\n",
    "window.grid_rowconfigure(0, weight=1)  # Let the textbox expand vertically\n",
    "window.grid_columnconfigure(0, weight=1)  # Let the entry box expand horizontally\n",
    "\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
